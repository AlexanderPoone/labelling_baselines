Pretrained lightweight baseline models on some large corpora (Wikipedia, [OSCAR](https://github.com/oscar-project/ungoliant), or Common Crawl):
* [asafaya/bert-large-arabic](https://github.com/alisafaya/Arabic-BERT), interestingly, by some guy from Koç University, Turkey    <- changed from 'base' to 'large'
* roberta-large    <- changed from 'base' to 'large'
* [dccuchile/albert-xxlarge-spanish](https://github.com/dccuchile/beto), aka BETO by DCC UChile    <- changed from `bert-base-spanish-wwm-cased` to `albert-xxlarge-spanish`
* camembert-large    <- changed from 'base' to 'large'
* [indolem/indobert-base-uncased](https://github.com/indolem/indolem), by some guys from UniMelb    <- there is no cased version
* bert-base-multilingual-cased    <- changed to cased
* bert-base-multilingual-cased    <- changed to cased
* [neuralmind/bert-large-portuguese-cased](https://github.com/neuralmind-ai/portuguese-bert), by a company named NeuralMind from São Paulo    <- changed from 'base' to 'large'
* bert-base-multilingual-cased    <- changed to cased
* [youscan/ukr-roberta-base](https://github.com/youscan/language-models), by a company named YouScan from Kyiv
* bert-base-chinese

It all boils down to `(time, country, evt, category), sent=(positive|negative|controversial|apathy))`.
